# LLM Security, Alignment & Governance Resources [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of research papers, experiments, and resources related to **LLM security and alignment** â€” including prompt injection, jailbreaks, hallucinations, defenses, governance, and ethical frameworks.  
Organized for reference and study.

**Last Updated:** 2025-09-17

---

## Legend

- â­ï¸ **Foundational** â€” Classic / seminal papers
- ğŸ›¡ï¸ **Practical** â€” Standards, guides, applied resources
- ğŸ§ª **Experimental** â€” New methods, ongoing research
- ğŸ“Š **Dataset/Benchmark** â€” Data resources, benchmarks
- ğŸ§¾ **Survey** â€” Reviews, surveys, taxonomies

---

## Citation Style Guide

- **arXiv preprints** â†’ `[arXiv:XXXX.XXXXX]`
- **Conference papers** â†’ `[VENUE YEAR]`
- **Journal articles** â†’ `[Journal Name, Year]`
- **Regulations & policy docs** â†’ `[Official Document ID]`
- **GitHub repos** â†’ `[GitHub]`
- **Blogs / Reports** â†’ `[Blog]` / `[Report]`

---

## Table of Contents

- [Prompt Injection & Jailbreaks](#prompt-injection--jailbreaks)
- [Hallucinations & Reliability](#hallucinations--reliability)
- [Defense Strategies](#defense-strategies)
- [Alignment & Safety](#alignment--safety)
- [Governance & Policy](#governance--policy)
- [Surveys & Overviews](#surveys--overviews)
- [Tools & Datasets](#tools--datasets)
- [Privacy & Data Security](#privacy--data-security)
- [Multimodal Security](#multimodal-security)
- [Model Cards (Major AI Labs)](#model-cards-major-ai-labs)
- [Other References](#other-references)

---

## Prompt Injection & Jailbreaks

1. Prompt Injection
- ğŸ§ª [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499) [arXiv:2306.05499] â€” Defines indirect injection attacks via external data sources; foundational work on LLM application vulnerabilities.
- ğŸ›¡ï¸ [Dropbox/llm-security](https://github.com/dropbox/llm-security) [GitHub] â€” Educational repo with demo code for injection attacks.
- ğŸ›¡ï¸ [OWASP Top 10 for LLMs](https://genai.owasp.org/llm-top-10/) [OWASP 2025] â€” Defines LLM01: Prompt Injection as the top security risk.
- ğŸ§ª [Backdoored Retrievers for Prompt Injection in RAG](https://arxiv.org/abs/2410.14479) [arXiv:2410.14479] â€” Poisoned retrievers in RAG pipelines enable indirect injection.
- ğŸ§ª [Manipulating LLM Web Agents via Indirect Injection](https://arxiv.org/abs/2507.14799) [arXiv:2507.14799] â€” Universal HTML triggers to hijack web agents.
- ğŸ“Š [WASP: Web Agent Security Benchmark](https://arxiv.org/abs/2504.18575) [arXiv:2504.18575] â€” Benchmarks agent robustness to indirect injections.

2. Jailbreaking / Adversarial Prompts
- ğŸ§ª [Universal and Transferable Adversarial Attacks on Aligned Language Models (v2)](https://arxiv.org/abs/2307.15043v2) [arXiv:2307.15043v2] â€” Updated version showing adversarial prompts transferable across multiple aligned LLMs, enabling generalized jailbreak attacks.
- ğŸ§ª [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483) [arXiv:2307.02483] â€” Anthropic analysis of safety training limitations.
- ğŸ§ª [Red Teaming Language Models to Reduce Harms](https://arxiv.org/abs/2209.07858) [arXiv:2209.07858] â€” Early research on red teaming methods for LLMs.
- ğŸ§ª [Many-shot Jailbreaking](https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf) [Anthropic 2024] â€” Shows long multi-shot contexts can bypass safety rules.
- ğŸ§ª [MASTERKEY: Automated Jailbreaking](https://www.ndss-symposium.org/ndss-paper/masterkey-automated-jailbreaking-of-large-language-model-chatbots/) [NDSS 2024] â€” Automated jailbreak generation.
- ğŸ§ª [White-box Multimodal Jailbreaks](https://arxiv.org/abs/2405.17894) [arXiv:2405.17894] â€” Vision-language jailbreaks using adversarial inputs.
- ğŸ§ª [Coordinated Prompt-RAG Attacks](https://arxiv.org/abs/2504.07717) [arXiv:2504.07717] â€” Coordinated poisoning of knowledge bases for RAG.
- ğŸ“Š [HarmBench](https://arxiv.org/abs/2402.04249) [arXiv:2402.04249] â€” Benchmark dataset for harmful content generation.
- ğŸ§ª [Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](https://arxiv.org/abs/2504.05652) [arXiv:2504.05652] â€” Stealth jailbreak method hiding malicious intent behind benign reasoning.
- ğŸ§ª [Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails](https://arxiv.org/abs/2504.11168) [arXiv:2504.11168] â€” Evasion techniques against commercial guardrail systems.
- ğŸ§ª [Subversion via Focal Points: Investigating Collusion in LLM Monitoring](https://arxiv.org/abs/2507.03010) [arXiv:2507.03010] â€” Models colluding to bypass monitoring protocols.
- ğŸ§ª [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks (v1)](https://arxiv.org/abs/2302.05733v1) [arXiv:2302.05733v1] â€” Demonstrates how LLMsâ€™ programmatic features can be misused via standard security attack techniques, highlighting dual-use risks and unexpected vulnerabilities.
- ğŸ§ª [Red Teaming the Mind of the Machine](https://arxiv.org/abs/2505.04806) [arXiv:2505.04806] â€” Systematic evaluation of 1,400+ adversarial prompts across major LLMs.

---

## Hallucinations & Reliability

- ğŸ§¾ [A Survey on Hallucination in LLMs](https://arxiv.org/abs/2311.05232) [arXiv:2311.05232] â€” Taxonomy, principles, and open questions on hallucinations.
- ğŸ§ª [SelfCheckGPT](https://arxiv.org/abs/2303.08896) [arXiv:2303.08896] â€” Self-verification technique for fact-checking model outputs.
- ğŸ§ª [RARR: Researching and Revising What LMs Say](https://arxiv.org/abs/2210.08726) [arXiv:2210.08726] â€” Detects and revises hallucinations in generated text.
- ğŸ“Š [TruthfulQA](https://arxiv.org/abs/2109.07958) [arXiv:2109.07958] â€” Benchmark measuring model truthfulness.
- ğŸ§ª [Does More Inference-Time Compute Really Help Robustness?](https://arxiv.org/abs/2507.15974) [arXiv:2507.15974] â€” Critical examination of inference-time scaling for robustness.

---

## Defense Strategies

- ğŸ§ª [Defending Against Prompt Injection With a Few DefensiveTokens](https://arxiv.org/abs/2507.07974) [arXiv:2507.07974] â€” Inserts defensive tokens at inference to block attacks.
- ğŸ›¡ï¸ [tldrsec/prompt-injection-defenses](https://github.com/tldrsec/prompt-injection-defenses) [GitHub] â€” Curated list of practical defense strategies.
- ğŸ›¡ï¸ [Llama Guard](https://arxiv.org/abs/2312.06674) [arXiv:2312.06674] â€” Meta's safety classifier for filtering harmful outputs.
- ğŸ§ª [LLMs Can Defend Themselves Against Jailbreaking](https://arxiv.org/abs/2406.05498) [arXiv:2406.05498] â€” Shadow stack approach for self-defense against jailbreaks.

---

## Alignment & Safety

- â­ï¸ [InstructGPT](https://arxiv.org/abs/2203.02155) [arXiv:2203.02155] â€” Introduced RLHF for instruction-following; foundation for GPT-3.5/4.
- â­ï¸ [Constitutional AI](https://arxiv.org/abs/2212.08073) [arXiv:2212.08073] â€” Anthropic's RLAIF: AI feedback guided by principles.
- â­ï¸ [Deep RL from Human Preferences](https://arxiv.org/abs/1706.03741) [arXiv:1706.03741] â€” Classic work on preference-based reward learning.
- ğŸ§¾ [Survey of RLHF](https://arxiv.org/abs/2312.14925) [arXiv:2312.14925] â€” Comprehensive review of RLHF techniques.
- ğŸ§ª [How Effective is Constitutional AI in Small LLMs?](https://arxiv.org/abs/2503.17365) [arXiv:2503.17365] â€” Tests CAI on smaller models.
- â­ï¸ [ARC Evals â†’ METR (Model Evaluation & Threat Research)](https://evals.alignment.org/) [Report] â€” Independent evaluations of frontier models (e.g. GPT-4) for potential dangerous capabilities. Formerly the evaluation team of ARC; now spun out as standalone nonprofit.
- ğŸ§ª [Sleeper Agents: Training Deceptive LLMs](https://arxiv.org/abs/2401.05566) [arXiv:2401.05566] â€” Demonstrates deceptive LLMs persisting through training.
- ğŸ§ª [Language Models Donâ€™t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://arxiv.org/abs/2305.04388) [arXiv:2305.04388] â€” Study showing that modelsâ€™ CoT explanations often donâ€™t reflect the true influences on outputs; can be misled by biasing features not mentioned in the explanation.
- ğŸ§ª [Explicit Vulnerability Generation with LLMs](https://arxiv.org/abs/2507.10054) [arXiv:2507.10054] â€” Investigation of LLMs generating insecure code when prompted.

---

## Governance & Policy

- ğŸ›¡ï¸ [EU AI Act](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32024R1689) [EU 2024] â€” Core EU regulation; includes requirements for General-Purpose AI (GPAI), risk classifications, transparency, and safety conditions for â€œhigh riskâ€ systems.  
- ğŸ›¡ï¸ [NIST AI RMF (2023)](https://www.nist.gov/itl/ai-risk-management-framework) â€” US voluntary framework for identifying, assessing, and managing AI risks over the lifecycle; includes a Generative AI Profile published in mid-2024.  
- ğŸ›¡ï¸ [OWASP Top 10 for LLMs](https://genai.owasp.org/llm-top-10/) â€” Industry standard list of major security threats specific to large language models.  

*Note: Regulatory / policy docs evolve fast â€” always check latest versions or drafts from official sources.*


---

## Surveys & Overviews

- ğŸ›¡ï¸ [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security) [GitHub] â€” Community-curated security resources.
- ğŸ›¡ï¸ [Awesome Jailbreak on LLMs](https://github.com/yueliu1999/Awesome-Jailbreak-on-LLMs) [GitHub] â€” Collection of state-of-the-art jailbreak methods.
- ğŸ§¾ [Prompt Hacking in LLMs 2024-2025 Literature Review](https://www.rohan-paul.com/p/prompt-hacking-in-llms-2024-2025) [Blog] â€” Comprehensive review of recent prompt hacking techniques.

---

## Tools & Datasets

- ğŸ“Š [sinanw/llm-security-prompt-injection](https://github.com/sinanw/llm-security-prompt-injection) [GitHub] â€” Dataset & experiments on prompt safety.
- ğŸ“Š [Open LLM Security Benchmark (NetSPI)](https://github.com/NetSPI/Open-LLM-Security-Benchmark?utm_source=chatgpt.com) â€” Benchmark framework evaluating both security (e.g. jailbreak resistance) and usability trade-offs in LLMs.
- ğŸ›¡ï¸ [Microsoft Presidio](https://github.com/microsoft/presidio) [GitHub] â€” Toolkit for PII detection and anonymization.
- ğŸ›¡ï¸ [OpenAI Moderation API](https://platform.openai.com/docs/guides/moderation) [Docs] â€” Content moderation endpoint and examples.
- ğŸ§ª [DefensiveToken Implementation](https://github.com/Sizhe-Chen/DefensiveToken) [GitHub] â€” Code for defensive token injection method.

---

## Privacy & Data Security

- ğŸ§ª [Extracting Training Data from LLMs](https://arxiv.org/abs/2012.07805) [arXiv:2012.07805] â€” Shows vulnerability to training data leakage.
- ğŸ§ª [Quantifying Memorization Across Neural LMs](https://arxiv.org/abs/2202.07646) [arXiv:2202.07646] â€” Quantifies memorization across model scales.

---

## Multimodal Security

- ğŸ§ª [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2306.13213) [arXiv:2306.13213] â€” Demonstrates that a single visual adversarial example can universally jailbreak aligned VLMs (e.g. MiniGPT-4, InstructBLIP, LLaVA), causing them to comply with harmful instructions they would normally refuse.

---

## ğŸ“‘ Model Cards (Major AI Labs)

### Anthropic
- ğŸ§ª [Claude 3 Family](https://www.anthropic.com/claude-3-model-card) [Report] â€” Safety evaluations and model details.
- ğŸ§ª Claude 4 (Opus/Sonnet) â€” Available via Claude.ai interface.

### OpenAI
- ğŸ›¡ï¸ [GPT-4o System Card](https://openai.com/index/gpt-4o-system-card/) [Report] â€” Multimodal safety considerations.
- ğŸ›¡ï¸ [o1 System Card](https://openai.com/index/openai-o1-system-card/) [Report] â€” Reasoning model safety evaluations.

### Google DeepMind
- ğŸ›¡ï¸ [Gemini Family Model Cards](https://ai.google.dev/gemma/docs) [Report] â€” Safety and capability assessments.

### Meta
- ğŸ›¡ï¸ [Llama 3 Model Card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md) [Report] â€” Open-source model safety details.

### Mistral AI
- ğŸ›¡ï¸ [Mistral Models Documentation](https://docs.mistral.ai/) [Report] â€” Model capabilities and safety measures.

---

## Other References

- ğŸ›¡ï¸ [Prompt Injection: What Is It and Why It Matters â€“ Simon Willison](https://simonwillison.net/2022/Sep/12/prompt-injection/) [Blog] â€” Early explanation of prompt injection risks.
- ğŸ§ª [Lakera: Gandalf â€“ The Prompt Injection Game](https://gandalf.lakera.ai/) [Game] â€” Interactive challenge for prompt injection.
- ğŸ›¡ï¸ [PortSwigger: Web LLM Attacks](https://portswigger.net/web-security/llm-attacks) [Blog] â€” Guide to prompt injection and related attacks.
- ğŸ›¡ï¸ [HiddenLayer: Prompt Injection Attacks on LLMs](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/) [Blog] â€” Comprehensive guide to LLM attacks and defenses.

---

**Maintainer:** 0xSweet  
**License:** CC BY 4.0


